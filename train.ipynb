{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import copy\n",
    "from video_dataset import VideoDataset, load_data\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from utils import trunc_normal_\n",
    "import torch\n",
    "import vision_transformer as vit\n",
    "from load_model import load_models\n",
    "import logging\n",
    "import os\n",
    "from video import generate_avi\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filename='training.log',\n",
    "    filemode='w'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded encoder\n",
      "Number of encoder parameters: 113998080\n",
      "Loaded predictor\n",
      "Number of predictor parameters: 213282816\n",
      "Loaded action_conditioner\n",
      "Number of action parameters: 42696960\n",
      "Loaded diffusion_model\n",
      "Number of parameters: 141838848\n",
      "Initializing encoder weights\n",
      "Initializing predictor weights\n",
      "Initializing action weights\n",
      "Initializing diffusion weights\n"
     ]
    }
   ],
   "source": [
    "encoder, predictor, action_conditioner, diffusion_model = load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 17 20:26:27 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060        Off | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   43C    P2              39W / 170W |   2605MiB / 12288MiB |     80%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1233      G   /usr/lib/xorg/Xorg                          171MiB |\n",
      "|    0   N/A  N/A      1453      G   /usr/bin/gnome-shell                         45MiB |\n",
      "|    0   N/A  N/A      4050      G   ...irefox/4090/usr/lib/firefox/firefox       15MiB |\n",
      "|    0   N/A  N/A      7901      G   ...ictureAPI --variations-seed-version       39MiB |\n",
      "|    0   N/A  N/A     10308      G   ...sion,SpareRendererForSitePerProcess      128MiB |\n",
      "|    0   N/A  N/A     16430      C   ...itHub/mc-world-model/env/bin/python     2188MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # load checkpoints from checkpoints_xd/name_weights_6.pt\n",
    "EPOCH = 155\n",
    "encoder.load_state_dict(torch.load(f'./checkpoints/{EPOCH}/encoder_weights_{EPOCH}.pt'))\n",
    "predictor.load_state_dict(torch.load(f'./checkpoints/{EPOCH}/predictor_weights_{EPOCH}.pt'))\n",
    "action_conditioner.load_state_dict(torch.load(f'./checkpoints/{EPOCH}/action_conditioner_weights_{EPOCH}.pt'))\n",
    "diffusion_model.load_state_dict(torch.load(f'./checkpoints/{EPOCH}/diffusion_model_weights_{EPOCH}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load optimizers/schedulers\n",
    "optimizer = torch.optim.AdamW(list(encoder.parameters()) + list(predictor.parameters()) + list(action_conditioner.parameters()), lr=3e-6)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "diffusion_optimizer = torch.optim.Adam(diffusion_model.parameters(), lr=3e-6)\n",
    "\n",
    "# Define the loss function\n",
    "def l2_loss(predictions, targets):\n",
    "    return torch.mean(torch.sqrt(torch.sum((predictions - targets) ** 2, dim=1)))\n",
    "\n",
    "# Define a MSE loss for the diffusion model\n",
    "def diffusion_loss(predictions, targets):\n",
    "    return F.mse_loss(predictions, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "epochs = 500\n",
    "ema_decay = 0.999  # EMA decay rate\n",
    "ema_encoder = copy.deepcopy(encoder)  # Create a copy of the encoder for EMA\n",
    "ema_encoder.to(device)\n",
    "video_num = 2268\n",
    "\n",
    "def update_ema(model, ema_model, decay):\n",
    "    with torch.no_grad():\n",
    "        for param, ema_param in zip(model.parameters(), ema_model.parameters()):\n",
    "            ema_param.data.mul_(decay).add_(param.data, alpha=1 - decay)\n",
    "\n",
    "accumulation_steps = 1\n",
    "for epoch in range(EPOCH+1, epochs):\n",
    "    # Load a new dataset\n",
    "    n_videos = 25\n",
    "    train_dataloader, test_dataloader = load_data(\n",
    "        data_folder='./datas/find-cave/', \n",
    "        start_idx=epoch*n_videos, \n",
    "        n_videos=n_videos, \n",
    "        action_sequence_length=25, \n",
    "        split_ratio=0.75, \n",
    "        batch_size=4,\n",
    "        num_workers=2,\n",
    "        shuffle=True,\n",
    "        frame_skip=1\n",
    "    )\n",
    "\n",
    "    # Use this training data 3 times for first 3, then 2 & increase n_videos from 3 to 5\n",
    "    for i in range(1):\n",
    "    \n",
    "        batch_loss = 0\n",
    "        for batch_idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "            frame_t, action_sequence, frame_tp1 = batch\n",
    "            frame_t = frame_t.to(device)\n",
    "            action_sequence = action_sequence.to(device)\n",
    "            frame_tp1 = frame_tp1.to(device)\n",
    "\n",
    "            encoder.train()\n",
    "            predictor.train()\n",
    "            action_conditioner.train()\n",
    "            diffusion_model.train()\n",
    "\n",
    "            # Forward pass for the JEPA model\n",
    "            x = encoder(frame_t)\n",
    "            x = action_conditioner(x, action_sequence)\n",
    "            x = predictor(x)\n",
    "\n",
    "            # Compute target using EMA encoder\n",
    "            with torch.no_grad():\n",
    "                y = ema_encoder(frame_tp1)\n",
    "                y = F.layer_norm(y, (y.size(-1),))\n",
    "            \n",
    "            # Compute loss for prediction network\n",
    "            loss_pred = l2_loss(x, y)\n",
    "            pstd_pred = torch.std(x, dim=1)  # Predictor variance across patches\n",
    "            loss_reg = torch.mean(F.relu(1. - pstd_pred))\n",
    "            reg_coeff = 0.0002  # Regularization coefficient\n",
    "            loss = loss_pred + reg_coeff * loss_reg\n",
    "\n",
    "            # Forward pass for the diffusion model\n",
    "            y = diffusion_model(y)\n",
    "            y = y.view(-1, 3, 224, 224)\n",
    "\n",
    "            # Compute loss for diffusion model\n",
    "            diff_loss = diffusion_loss(y, frame_tp1)\n",
    "\n",
    "            # Accumulate gradients\n",
    "            loss = loss / accumulation_steps\n",
    "            diff_loss = diff_loss / accumulation_steps\n",
    "            loss.backward()\n",
    "            diff_loss.backward()\n",
    "\n",
    "            # Perform optimization step after accumulating gradients\n",
    "            if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                diffusion_optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                diffusion_optimizer.zero_grad()\n",
    "        \n",
    "            # Update EMA encoder\n",
    "            update_ema(encoder, ema_encoder, ema_decay)\n",
    "            # Print loss for every 5th batch\n",
    "            if batch_idx % 100 == 0:\n",
    "                logger.info(f\"Epoch [{epoch+1}/{epochs}], Batch [{batch_idx}/{len(train_dataloader)}], JEPA Loss: {loss.item() * accumulation_steps:.4f}\")\n",
    "                logger.info(f\"Epoch [{epoch+1}/{epochs}], Batch [{batch_idx}/{len(train_dataloader)}], Diffusion Loss: {diff_loss.item() * accumulation_steps:.4f}\")\n",
    "            \n",
    "            if batch_idx % 500 == 0 and video_num != 235:\n",
    "                generate_avi(encoder, predictor, action_conditioner, diffusion_model, video_num, path=f'./output_video_{epoch}_{batch_idx}.avi')\n",
    "                logger.info(f'new_vid at {video_num}')\n",
    "                video_num += 1\n",
    "    \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "    # Print progress\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        logger.info(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {loss.item() * accumulation_steps:.4f}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(\"saving checkpoints\")\n",
    "        if not os.path.exists(f'./checkpoints/{epoch}/'):\n",
    "            os.makedirs(f'./checkpoints/{epoch}/')\n",
    "        torch.save(encoder.state_dict(), f'./checkpoints/{epoch}/encoder_weights_{epoch}.pt')\n",
    "        torch.save(predictor.state_dict(), f'./checkpoints/{epoch}/predictor_weights_{epoch}.pt')\n",
    "        torch.save(action_conditioner.state_dict(), f'./checkpoints/{epoch}/action_conditioner_weights_{epoch}.pt')\n",
    "        torch.save(diffusion_model.state_dict(), f'./checkpoints/{epoch}/diffusion_model_weights_{epoch}.pt')\n",
    "\n",
    "        logger.info('Saved checkpoints.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
